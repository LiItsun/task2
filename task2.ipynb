{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic impoort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yukino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams, FreqDist\n",
    "from nltk.lm.models import LanguageModel\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download lyrics of Eminem and JAY-Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "\n",
    "genius = lyricsgenius.Genius(\"dCk-OdsTTfxGlwtWYOfkAgUoTadJoiJCur7l2sX41AlI85vJufciDfSrS79J_uA-\")\n",
    "\n",
    "genius.verbose = False\n",
    "genius.remove_section_headers = True\n",
    "genius.skip_non_songs = False\n",
    "genius.excluded_terms = [\"(Remix)\", \"(Live)\"]\n",
    "\n",
    "artist = genius.search_artist(\"Eminem\", max_songs=15, sort=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('12 Days of Diss-Mas', 'Eminem'), ('1-833-2GET-REV (REVIVAL Voicemail)', 'Eminem'), ('1996 Underground Freestyle', 'Eminem'), ('1997 Freestyle Live at Wetlands, NYC', 'Eminem'), ('1997 Rap Olympics', 'Eminem'), ('1998 Wake Up Show Freestyle', 'Eminem'), ('1999', 'Eminem'), ('1999 Tim Westwood Freestyle', 'Eminem'), ('2004 Tim Westwood Freestyle', 'Eminem'), ('2012 Something From Nothing: Art of Rap Freestyle', 'Eminem'), ('2.0 Boys', 'Eminem'), ('25 To Life', 'Eminem'), ('313', 'Eminem'), ('3 a.m.', 'Eminem'), ('3 Verses', 'Eminem')]\n"
     ]
    }
   ],
   "source": [
    "print(artist.songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote `Lyrics_Eminem.json`\n"
     ]
    }
   ],
   "source": [
    "artist.save_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius = lyricsgenius.Genius(\"dCk-OdsTTfxGlwtWYOfkAgUoTadJoiJCur7l2sX41AlI85vJufciDfSrS79J_uA-\")\n",
    "\n",
    "genius.verbose = False\n",
    "genius.remove_section_headers = True\n",
    "genius.skip_non_songs = False\n",
    "genius.excluded_terms = [\"(Remix)\", \"(Live)\"]\n",
    "\n",
    "artist2 = genius.search_artist(\"JAY-Z\", max_songs=15, sort=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('???', 'JAY-Z'), ('’03 Bonnie & Clyde', 'JAY-Z'), ('100$ Bill', 'JAY-Z'), ('1-900-Hustler', 'JAY-Z'), ('1999 Tim Westwood Freestyle', 'JAY-Z'), ('1999 Westwood Freestyle', 'JAY-Z'), ('2002 Funk Flex Freestyle', 'JAY-Z'), ('20 Bag Shorty', 'JAY-Z'), ('22 Days Challenge', 'JAY-Z'), ('22 Two’s', 'JAY-Z'), ('2 Many Hoes', 'JAY-Z'), ('30 Something', 'JAY-Z'), ('30 Something (Remix)', 'JAY-Z'), ('4:44', 'JAY-Z'), ('44 Four’s', 'JAY-Z')]\n"
     ]
    }
   ],
   "source": [
    "print(artist2.songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote `Lyrics_JAYZ.json`\n"
     ]
    }
   ],
   "source": [
    "artist2.save_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_Eminem.json'\n",
    "with open(file) as train_file:\n",
    "    dict_train = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Lyrics_JAYZ.json'\n",
    "with open(file) as train_file:\n",
    "    jayz_dict = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_trainsong = dict_train.get('songs')\n",
    "jayz_songs = jayz_dict.get('songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lyrics = []\n",
    "for i in range(0,len(dict_trainsong)):\n",
    "    idict = dict_trainsong[i]\n",
    "    song_lyric = idict.get('lyrics')\n",
    "    num_lyrics.append(song_lyric)\n",
    "\n",
    "len(num_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out the lyrics with no NA\n",
    "df = pd.DataFrame(num_lyrics) \n",
    "df = df.rename(columns={0: \"lyrics\"})\n",
    "df = df[df['lyrics'].notna()]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_jayz_lyrics = []\n",
    "for i in range(0,len(jayz_songs)):\n",
    "    idic = jayz_songs[i]\n",
    "    song_lyric = idict.get('lyrics')\n",
    "    num_jayz_lyrics.append(song_lyric)\n",
    "    \n",
    "len(num_jayz_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jayz = pd.DataFrame(num_jayz_lyrics) \n",
    "df_jayz = df_jayz.rename(columns={0: \"lyrics\"})\n",
    "df_jayz = df_jayz[df_jayz['lyrics'].notna()]\n",
    "len(df_jayz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics\n",
       "0  It's like this and like that and like this\\nIt...\n",
       "1  It's like this and like that and like this\\nIt...\n",
       "2  It's like this and like that and like this\\nIt...\n",
       "3  It's like this and like that and like this\\nIt...\n",
       "4  It's like this and like that and like this\\nIt..."
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jayz = df_jayz.replace(to_replace ='[\\(\\[].*?[\\)\\]]', value = '', regex = True)\n",
    "df_jayz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['!','\"','#','$','%','&','(',')','*','+',',','-','/',':',';','<','=','>','?','@','[','\\\\',']',\n",
    " '^','_','`','{','|','}','~']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(y):\n",
    "    for i in range(0, len(y)):        \n",
    "        stringlyric = df.iloc[i,0]\n",
    "        stringlyric.replace(\"\\n\\n\\n\\n\\n\\n\\n\\n\", \"\\n\")\n",
    "        stringlyric.replace(\"\\n\\n\\n\\n\\n\\n\\n\", \"\\n\")\n",
    "        stringlyric.replace(\"\\n\\n\\n\\n\\n\\n\", \"\\n\")\n",
    "        stringlyric.replace(\"\\n\\n\\n\\n\\n\", \"\\n\")\n",
    "        stringlyric.replace(\"\\n\\n\\n\\n\", \"\\n\")\n",
    "        stringlyric.replace(\"\\n\\n\\n\", \"\\n\")\n",
    "        stringlyric.replace(\"\\n\\n\", \"\\n\")\n",
    "        for x in punctuation: # remove bad_chars  \n",
    "            stringlyric = stringlyric.replace(x, '')\n",
    "        stringlyric = stringlyric.lower()\n",
    "        df.iloc[i,0] = stringlyric \n",
    "        return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. hit 'em up by 2pac ft. outlawz\\n2. diss fe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thank you for your interest in revival the num...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>any rapper saying those kind of rhymes\\nin thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>everybody duck down all you hear is the sound\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i'ma tell you this for your own benefit\\nyour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>broke the rubber\\nbusted a nut up in your moth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hi kids do you like kim's bullshit no\\nwanna s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tim westwood marley marl and slim shady and mc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ryan's a homicidal misfit i write the solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>too late for the other side\\ncaught in a chase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>now what you know about a sweet mc from the 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>there is no escaping yo\\nthere's no place to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>it's like this and like that and like this\\nit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lyrics\n",
       "0   1. hit 'em up by 2pac ft. outlawz\\n2. diss fe ...\n",
       "1   thank you for your interest in revival the num...\n",
       "2   any rapper saying those kind of rhymes\\nin thi...\n",
       "3   everybody duck down all you hear is the sound\\...\n",
       "4   i'ma tell you this for your own benefit\\nyour ...\n",
       "5   broke the rubber\\nbusted a nut up in your moth...\n",
       "6   hi kids do you like kim's bullshit no\\nwanna s...\n",
       "7   tim westwood marley marl and slim shady and mc...\n",
       "10  ryan's a homicidal misfit i write the solution...\n",
       "11  too late for the other side\\ncaught in a chase...\n",
       "12  now what you know about a sweet mc from the 31...\n",
       "13  there is no escaping yo\\nthere's no place to h...\n",
       "14  it's like this and like that and like this\\nit..."
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>It's like this and like that and like this\\nIt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lyrics\n",
       "0   It's like this and like that and like this\\nIt...\n",
       "1   It's like this and like that and like this\\nIt...\n",
       "2   It's like this and like that and like this\\nIt...\n",
       "3   It's like this and like that and like this\\nIt...\n",
       "4   It's like this and like that and like this\\nIt...\n",
       "5   It's like this and like that and like this\\nIt...\n",
       "6   It's like this and like that and like this\\nIt...\n",
       "7   It's like this and like that and like this\\nIt...\n",
       "8   It's like this and like that and like this\\nIt...\n",
       "9   It's like this and like that and like this\\nIt...\n",
       "10  It's like this and like that and like this\\nIt...\n",
       "11  It's like this and like that and like this\\nIt...\n",
       "12  It's like this and like that and like this\\nIt...\n",
       "13  It's like this and like that and like this\\nIt...\n",
       "14  It's like this and like that and like this\\nIt..."
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove(df_jayz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df\n",
    "df1['lyrics'] = df1['lyrics'].str.lower()\n",
    "df1['tokenized_sents'] = df1.apply(lambda row: nltk.word_tokenize(row['lyrics']), axis=1)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jayz['tokenized_sents'] = df_jayz.apply(lambda row: nltk.word_tokenize(row['lyrics']), axis=1)\n",
    "len(df_jayz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put lyrics together and Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm.preprocessing import flatten\n",
    "train, test = train_test_split(df1, test_size=0.2, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train['lyrics']\n",
    "test_sentences = test['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences_list = train_sentences.tolist()\n",
    "len(train_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train['tokenized_sents']\n",
    "train_jayz = df_jayz['tokenized_sents']\n",
    "test = test['tokenized_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine training data from Eminem dataset and JAYZ\n",
    "token_train = train.tolist()\n",
    "token_train_jayz = train_jayz.tolist()\n",
    "token_train.extend(token_train_jayz)\n",
    "len(token_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16918"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_train = []\n",
    "for sublist in token_train:\n",
    "    for token in sublist:\n",
    "        tokens_train.append(token)\n",
    "        \n",
    "len(tokens_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1350"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_te = list(test_sentences.apply(nltk.word_tokenize))\n",
    "tokens_test = []\n",
    "for sublist in tokens_te:\n",
    "    for token in sublist:\n",
    "        tokens_test.append(token)\n",
    "len(tokens_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with 2 models\n",
    "## This part was helped by my classmate Li Xiran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramsDist = FreqDist()\n",
    "for i in tokens_train:\n",
    "    sWordFreq = FreqDist(word_tokenize(i))\n",
    "    for j in sWordFreq:\n",
    "        if j in unigramsDist:\n",
    "            unigramsDist[j] += sWordFreq[j]\n",
    "        else:\n",
    "            unigramsDist[j] = sWordFreq[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tokens_test:\n",
    "    word = word_tokenize(i)\n",
    "    for j in word:\n",
    "        if j not in unigramsDist:\n",
    "            unigramsDist[j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = unigramsDist.N() + unigramsDist.B()\n",
    "unigramsFreq = FreqDist()\n",
    "for i in unigramsDist:\n",
    "    unigramsFreq[i] = (unigramsDist[i] + 1) / smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt = []\n",
    "for sentence in test_data:\n",
    "    logprob = 0\n",
    "    wt = 0\n",
    "    for word in word_tokenize(sentence):\n",
    "        if word in unigramsFreq:\n",
    "            logprob += log(unigramsFreq[word],2)\n",
    "            wt += 1\n",
    "    if wt > 0:\n",
    "        ppt.append([sentence,pow(2,-(logprob/wt))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534.6386183663429\n"
     ]
    }
   ],
   "source": [
    "temp = 0\n",
    "for i in ppt:\n",
    "    temp += i[1]\n",
    "print(temp/len(ppt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = {}\n",
    "bigramsDist = FreqDist()\n",
    "\n",
    "for sentence in train_data:\n",
    "    sWordFreq = FreqDist(bigrams(word_tokenize(sentence)))\n",
    "    for j in sWordFreq:\n",
    "        if j in bigramsDist:\n",
    "            bigramsDist[j] += sWordFreq[j]\n",
    "        else:\n",
    "            bigramsDist[j] = sWordFreq[j]\n",
    "            if j[0] in w2gram:\n",
    "                bigram[j[0]] += 1\n",
    "            else:\n",
    "                bigram[j[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in test_data:\n",
    "    word = bigrams(word_tokenize(sentence))\n",
    "    for j in word:\n",
    "        if j not in bigramsDist:\n",
    "            bigramsDist[j] = 0\n",
    "            \n",
    "            if j[0] in w2gram:\n",
    "                bigram[j[0]] += 1\n",
    "            else:\n",
    "                bigram[j[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "for i in bigramsDist:\n",
    "    if i[0] in history:\n",
    "        history[i[0]] += bigramsDist[i]\n",
    "    else:\n",
    "        history[i[0]] = bigramsDist[i]\n",
    "bigramsFreq = FreqDist()\n",
    "for i in bigramsDist:\n",
    "    bigramsFreq[i] = (bigramsDist[i] + 1) / (history[i[0]] + bigram[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt2 = []\n",
    "for sentence in test_data:\n",
    "    logprob = 0\n",
    "    wt = 0\n",
    "    for word in bigrams(word_tokenize(sentence)):\n",
    "        if word in bigramsFreq:\n",
    "            logprob += log(bigramsFreq[word],2)\n",
    "            wt += 1\n",
    "    if wt > 0:\n",
    "        ppt2.append([sentence,pow(2,-(logprob/wt))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.66937768365333\n"
     ]
    }
   ],
   "source": [
    "temp2 = 0\n",
    "for i in ppt2:\n",
    "    temp2 += i[1]\n",
    "print(temp2/len(ppt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the result of perplexity of two models, we are chosing unigram as the model of generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat a generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedLanguageModel(LanguageModel):\n",
    "    \"\"\"Logic common to all interpolated language models.\n",
    "    The idea to abstract this comes from Chen & Goodman 1995.\n",
    "    Do not instantiate this class directly!\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing_cls, order, **kwargs):\n",
    "        assert issubclass(smoothing_cls, Smoothing)\n",
    "        params = kwargs.pop(\"params\", {})\n",
    "        super().__init__(order, **kwargs)\n",
    "        self.estimator = smoothing_cls(self.vocab, self.counts, **params)\n",
    "\n",
    "    def unmasked_score(self, word, context=None):\n",
    "        if not context:\n",
    "                return self.estimator.unigram_score(word)\n",
    "        if not self.counts[context]:\n",
    "#This conversation was marked as resolved by stevenbird  Show conversation\n",
    "             # It can also happen that we have no data for this context.\n",
    "             # In that case we defer to the lower-order ngram.\n",
    "             # This is the same as setting alpha to 0 and gamma to 1.\n",
    "             return self.unmasked_score(word, context[1:])\n",
    "        alpha, gamma = self.estimator.alpha_gamma(word, context)\n",
    "        return alpha + gamma * self.unmasked_score(word, context[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.models import Smoothing\n",
    "class KneserNey(Smoothing):\n",
    "    def __init__(self, vocabulary, counter, discount=0.9, **kwargs):\n",
    "        super(KneserNey, self).__init__(vocabulary, counter, *kwargs)\n",
    "        super().__init__(vocabulary, counter, **kwargs)\n",
    "        self.discount = discount\n",
    "\n",
    "    def unigram_score(self, word):\n",
    "        return 1.0 / len(self.vocab)\n",
    "\n",
    "    def alpha_gamma(self, word, context):\n",
    "        prefix_counts = self.counts[context]\n",
    "        prefix_total_ngrams = prefix_counts.N()\n",
    "        alpha = max(prefix_counts[word] - self.discount, 0.0) / prefix_total_ngrams\n",
    "        gamma = self.discount * _count_non_zero_vals(prefix_counts) / prefix_total_ngrams\n",
    "        return alpha, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneserNeyInterpolated(InterpolatedLanguageModel):\n",
    "    def __init__(self, order, discount=0.9, **kwargs):\n",
    "        super().__init__(KneserNey, order, params={\"discount\": discount}, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "padded_Line1 = [list(pad_both_ends(tokens_train, n))]\n",
    "train_data1, padded_sents1 = padded_everygram_pipeline(n, padded_Line1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = KneserNeyInterpolated(1, discount = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1841 items>\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train_data1, padded_sents1)\n",
    "print(model1.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "def generate_sent(model, num_words, random_seed): #will generate sentences\n",
    "    content = []\n",
    "    period = '.' # finish the sentences\n",
    "    words = 0\n",
    "    while words < num_words:\n",
    "        for token in model.generate(num_words, random_seed=random_seed):\n",
    "            if token == '<s>':\n",
    "                continue\n",
    "            if token == '</s>':\n",
    "                content.append(period)\n",
    "                break\n",
    "            words += 1\n",
    "            content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(model, lines, length, length_range):\n",
    "    song = []\n",
    "    for _ in range(lines):\n",
    "        sent_len = np.random.randint(*length_range) if length_range else length\n",
    "        i = np.random.randint(1,100)\n",
    "        text = generate_sent(model, num_words=sent_len, random_seed=i)\n",
    "        song.append(text)\n",
    "    return '\\n'.join(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conditions arson g blunt accord gentleman try skillet\\nsicker sold inhaling cussin! paper house ryan\\npsychotic spontaneous bus zone case paul and rupaul\\nweed best 9th – but bad ottoman failed thinkin\\nGot pocket biophysics homie pause sicker hihat jesus 3\\ni owe pass biggity, follow death slurping\\nsmashing anymore umbilical read cross guzzlin Driving black ottoman\\npsychotic spontaneous bus zone case paul and rupaul\\nlock beat pry dolly vietnam carpets said bottom where\\nadmission saw will goodbye wake hair nayhood tryna\\nalive broke phone boom wanted standin rigging hooligan\\ncorner joint borders softening coma loosies unintelligible gang whip\\nfloating war chain distance grade confusion dogg squad invited\\nask popping oughta violent date crew remember paid\\nchildhood friend catch f— machine old credit felt\\nshrinkin Coming spittin dump 12 acting joint nobody softening\\nnumber hahaha raise acappella pole follow mouth brewery\\nchildhood friend catch f— machine old credit felt bullet\\nfoot unlimited staircase chop symptoms not Kevorkian wasted crew\\ngang canal today new laugh pocket sticking piece\\ndramatically meaner daily slow horrible everybody tight sick strapped\\nburned panther evil… cellar invasive jackers illin hoist d12\\nclown wept bang potions allergic could ‘ chicken\\nalive broke phone boom wanted standin rigging hooligan sore\\nKevorkian mind-state 3 kim without directly window nostradamus that'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_Eminem = generator(model1, lines=25, length = 8, length_range=[8, 10])\n",
    "new_Eminem.split(\"\\n\")\n",
    "new_Eminem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
